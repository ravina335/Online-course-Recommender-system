# -*- coding: utf-8 -*-
"""bow_practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1111TmgNZcsCB1P-fKnB-sY2DgWDHkeQS
"""

!pip install nltk==3.6.7
!pip install gensim
!pip install scipy==1.10
!pip install pandas
!pip install nltk
!pip install matplotlib

import gensim
import pandas as pd
import nltk as nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim import corpora

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

course1 = "this is an introduction data science course which introduces data science to beginners"

course2 = "machine learning for beginners"
courses = [course1, course2]
courses

tokenized_courses = [word_tokenize(course) for course in courses]
tokenized_courses



tokens_dict = gensim.corpora.Dictionary(tokenized_courses)
print(tokens_dict.token2id)

courses_bow = [tokens_dict.doc2bow(course) for course in tokenized_courses]
courses_bow

stop_words = set(stopwords.words('english'))
tokenized_courses[0]
processed_tokens = [w for w in tokenized_courses[0] if not w.lower() in stop_words]
processed_tokens

tags = nltk.pos_tag(tokenized_courses[0])
tags

course_url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/course_processed.csv"
course_content_df = pd.read_csv(course_url)
course_content_df.head()

course_content_df['course_texts'] = course_content_df[['TITLE', 'DESCRIPTION']].agg(' '.join, axis=1)
course_content_df.head()

def tokenize_course(course, keep_only_nouns=True):
    # Get English stop words
    stop_words = set(stopwords.words('english'))
    # Tokenize the course text
    word_tokens = word_tokenize(course)
    # Remove English stop words and numbers
    word_tokens = [w for w in word_tokens if (not w.lower() in stop_words) and (not w.isnumeric())]
    # Only keep nouns
    if keep_only_nouns:
        # Define a filter list of non-noun POS tags
        filter_list = ['WDT', 'WP', 'WRB', 'FW', 'IN', 'JJR', 'JJS', 'MD', 'PDT', 'POS', 'PRP', 'RB', 'RBR', 'RBS',
                       'RP']
        # Tag the word tokens with POS tags
        tags = nltk.pos_tag(word_tokens)
        # Filter out non-nouns based on POS tags
        word_tokens = [word for word, pos in tags if pos not in filter_list]

    return word_tokens

a_course = course_content_df.iloc[0, :]['course_texts']
a_course

# prompt: make for loop to execute the course text for tokenizecourse

for index, row in course_content_df.iterrows():
  course_text = row['course_texts']
  tokenized_course = tokenize_course(course_text)
  print(f"Course_text: {course_text[:50]}... , Tokenized: {tokenized_course[:10]}...")

# prompt: apply doc2bow function along with dictonary on each row of tokenizeed_course
def create_bow_and_return_dict(tokenized_course):
    tokens_dict = gensim.corpora.Dictionary([tokenized_course]) # Create dictionary from the tokenized course
    bow_corpus = tokens_dict.doc2bow(tokenized_course) # Generate bag-of-words representation
    return tokens_dict, bow_corpus

course_content_df['bow_corpus'] = None # Initialize 'bow_corpus' column to None
course_content_df['tokens_dict'] = None # Initialize 'tokens_dict' column to None

for index, row in course_content_df.iterrows():
  course_text = row['course_texts']
  tokenized_course = tokenize_course(course_text)
  tokens_dict, bow_corpus = create_bow_and_return_dict(tokenized_course)

  # Convert bow_corpus and tokens_dict to strings before storing them
  course_content_df.loc[index, 'bow_corpus'] = str(bow_corpus)
  course_content_df.loc[index, 'tokens_dict'] = str(tokens_dict)

# Assuming you want to show the Bag-of-Words (bow_corpus) for each course

for index, row in course_content_df.iterrows():
  print(f"Course Title: {row['TITLE']}")
  print(f"Bag-of-Words Representation: {row['bow_corpus']}")
  print("-" * 30)

doc_indices = []
doc_ids = []
tokens = []
bow_values = []

for token_index, token_bow in bow_corpus:  # Directly iterate over bow_corpus
    doc_indices.append(0)  # Assuming a single document, index is 0
    # Make sure 'courses_df' is defined and accessible here
    # You might need to adjust the indexing for 'courses_df'
    doc_ids.append(course_content_df.iloc[0]['COURSE_ID'])
    tokens.append(tokens_dict[token_index])  # Get the token (word) using the token_index
    bow_values.append(token_bow)  # Get the BoW value (frequency of the word in the document)

bow_dicts = {
    "doc_index": doc_indices,
    "doc_id": doc_ids,
    "token": tokens,
    "bow": bow_values
}

# Create the DataFrame
course_bow = pd.DataFrame(bow_dicts)

# Display the resulting DataFrame
print(course_bow)